# -*- coding: utf-8 -*-
"""second_pass.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-sEcTY6Wvi0Qeis63B8mDHC-pbT5sNIT
"""

import random
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, activations
from matplotlib import pyplot as plt
import scipy
from tqdm import tqdm
import math
import numpy as np
from sklearn.model_selection import train_test_split

runtime = 'native' # alternatively 'colab'

if runtime == 'colab':
    from google.colab import drive
    drive.mount('/content/drive')

class ConvBlock(layers.Layer):

  def __init__(
      self,
      out_size=(32, 32),
      dynamic_kernel_sizing=False,
      kernel_size=(3, 3),
      filters=16,
      batch_norm_present=True,
      max_pool_present=False,
      noise_present=False,
      stddev=0.05,
      activation=activations.relu
    ):
    super().__init__()
    self.out_size = out_size
    self.dynamic_kernel_sizing = dynamic_kernel_sizing
    self.kernel_size = kernel_size
    self.filters = filters

    self.batch_norm_present = batch_norm_present
    self.batch_norm = layers.Activation(activations.linear)
    if self.batch_norm_present:
      self.batch_norm = layers.BatchNormalization(axis=-1)

    self.noise_present = noise_present
    self.gaussian_noise = layers.Activation(activations.linear)
    if self.noise_present:
      self.gaussian_noise = layers.GaussianNoise(stddev)

    self.max_pool_present = max_pool_present
    self.max_pool = layers.Activation(activations.linear)
    if self.max_pool_present:
      # Sets kernel AND stride to (2, 2)
      self.max_pool = layers.MaxPool2D(pool_size=2)

    # Convolutional layer will be edited in build to match kernel size based on image and desired output shape
    self.conv = layers.Conv2D(self.filters, self.kernel_size, strides=(1, 1), padding='same')
    self.activation = layers.Activation(activation)

  # DYNAMIC KERNEL SIZING (requires out_size parameter in __init__)
  def build(self, inputs_shape):
    if self.dynamic_kernel_sizing:
      img_dim = inputs_shape[1]
      #print(type(img_dim), img_dim)
      pad_mode = 'same'

      if not self.max_pool_present:
        self.kernel_size = img_dim + 1 - self.out_size[0]
        pad_mode = 'valid'
      else:
        # With max pooling enabled, desired output size must be less than half of the input image size
        mp_out_size = math.floor((img_dim - 2) / 2) + 1
        self.kernel_size = mp_out_size + 1 - self.out_size[0]

      self.conv = layers.Conv2D(self.filters, (self.kernel_size, self.kernel_size), strides=(1, 1), padding=pad_mode)


  def call(self, inputs, training=None):
    x = self.batch_norm(inputs)
    if training:
      x = self.gaussian_noise(x)
    x = self.max_pool(x)
    x = self.conv(x)
    x = self.activation(x)
    return x

class MLPBlock(layers.Layer):
  def __init__(
      self,
      structure=[10],
      dropout_present=True,
      dropout_rate=0.1,
      activation=activations.relu,
      classification_mode=False
    ):
    super().__init__()
    if isinstance(structure, (list, tuple)):
      self.structure = structure
    else:
      self.structure = [10]
    self.flatten = layers.Flatten()
    self.dropout_present = dropout_present
    self.dropout_rate = dropout_rate
    self.dropout = layers.Activation(activations.linear)
    if self.dropout_present:
      self.dropout = layers.Dropout(self.dropout_rate)
    self.activation = layers.Activation(activation)
    self.classification_mode = classification_mode

    # temp fix for the _training error
    self.dense_1 = layers.Dense(64)
    self.dense_2 = layers.Dense(32)
    self.dense_3 = layers.Dense(16)
    self.dense_4 = layers.Dense(10)

  def call(self, inputs, training=None):
    x = self.flatten(inputs)

    x = self.dense_1(x)
    x = self.dropout(x)
    x = self.activation(x)
    x = self.dense_2(x)
    x = self.dropout(x)
    x = self.activation(x)
    x = self.dense_3(x)
    x = self.dropout(x)
    x = self.activation(x)
    x = self.dense_4(x)

    # # commented out to hard code a test fix for the _training error before
    # # resorting to functional implementation
    # for i, units in enumerate(self.structure):
    #   x = layers.Dense(units)(x)
    #   if i + 1 < len(self.structure):
    #     if training:
    #       x = layers.Dropout(self.dropout_rate)(x)
    #     x = self.activation(x)
    if self.classification_mode:
      x = layers.Activation(activations.softmax)(x)
    return x

class CNN(keras.Model):
  def __init__(self):
    super().__init__()
    self.CNN_Block_1 = ConvBlock(
        out_size = (64, 64),
        dynamic_kernel_sizing=True,
        filters=16,
        batch_norm_present=True,
        max_pool_present=False,
        noise_present=True
    )
    self.CNN_Block_2 = ConvBlock(
        kernel_size = (3, 3),
        filters=32,
        batch_norm_present=True,
        max_pool_present=True,
        noise_present=True
    )
    self.CNN_Block_3 = ConvBlock(
        kernel_size = (3, 3),
        filters=32,
        batch_norm_present=True,
        max_pool_present=True,
        noise_present=True
    )
    self.CNN_Block_4 = ConvBlock(
        kernel_size = (3, 3),
        filters=16,
        batch_norm_present=True,
        max_pool_present=True,
        noise_present=True
    )
    self.MLP_Block_1 = MLPBlock()#structure=(64, 32, 16, 10))
    self.flatten = layers.Flatten()

  def call(self, inputs):
    # print(inputs, type(inputs))
    # print(len(inputs))
    # print(inputs[0])
    # print(inputs[1])
    img, info = inputs
    c = self.CNN_Block_1(img)
    c = self.CNN_Block_2(c)
    c = self.CNN_Block_3(c)
    c = self.CNN_Block_4(c)

    c = self.flatten(c)
    # Stacks the flattened image tensor vertically, ignoring the batch, with the
    # 1D vertical tensor containing the additional information
    c = tf.concat((c, info), axis=1)

    out = self.MLP_Block_1(c)

    return out

# def plot_history():
#     """
#     Copied from app_d/ml.py
#     History object is returned by the fit function
#     """
#     with open("history.pkl", "rb") as file:
#         history = pickle.load(file)
#     loss, acc, val_loss, val_acc = list(history.values())
#     start = 50  # at which epoch the plot should start
#     plt.plot(range(start, len(loss)), loss[start:], "r", label="training loss")
#     plt.plot(
#         range(start, len(val_loss)), val_loss[start:], "b", label="validation loss"
#     )
#     plt.legend()
#     plt.grid(True)
#     plt.xlabel("Epochs")
#     plt.ylabel("Loss")
#     plt.show()
#     plt.plot(range(1, len(acc) + 1), acc, "r", label="training accuracy")
#     plt.plot(range(1, len(val_acc) + 1), val_acc, "b", label="validation accuracy")
#     plt.legend()
#     plt.grid(True)
#     plt.xlabel("Epochs")
#     plt.ylabel("Accuracy")
#     plt.show()

with tf.device('/GPU:0'):
  if runtime == 'colab':
    selected_path = '/content/drive/MyDrive/MISTI/MISTI24/NewData/data.npz'
  else:
    selected_path = '/home/Package/data/data-round-1.npz'
  with np.load(selected_path) as data:
    data_X1 = np.moveaxis(data['images'], 1, -1)
    data_X2 = data['inputs'][:, 1:3].astype(np.float32)
    data_Y = data['onehots']
    X1_train, X1_test, X2_train, X2_test, y_train, y_test = train_test_split(
        data_X1, data_X2, data_Y,
        test_size=0.2,
        shuffle=True
    )
    dataset_train = tf.data.Dataset.from_tensor_slices(((X1_train, X2_train), y_train)).batch(batch_size=64, drop_remainder=True).prefetch(tf.data.AUTOTUNE)
    dataset_test = tf.data.Dataset.from_tensor_slices(((X1_test, X2_test), y_test)).batch(batch_size=1, drop_remainder=True).prefetch(tf.data.AUTOTUNE)

with tf.device('/GPU:0'):
  cnn = CNN()
  optimizer = keras.optimizers.Adam(learning_rate=1e-3)
  loss = keras.losses.CategoricalCrossentropy(
      from_logits=False,
      label_smoothing=0.0,
      axis=-1,
      reduction='sum_over_batch_size',
      name='categorical_crossentropy'
  )
  metric = keras.metrics.CategoricalCrossentropy(
      dtype=None,
      from_logits=False,
      label_smoothing=0,
      axis=-1,
      name='categorical_crossentropy'
  )
  cnn.compile(optimizer=optimizer, loss=loss, metrics=[metric])

with tf.device('/GPU:0'):
  early_stopping_cb = keras.callbacks.EarlyStopping(
      patience=20, restore_best_weights=True
  )

  hist = cnn.fit(
      dataset_train,
      validation_data=dataset_test,
      epochs=200,
      callbacks=[early_stopping_cb]
  )